import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import transforms
import os
from PIL import Image
from dataloader import GhibliDataset
from model import GAN

# Assuming GAN, Encoder, Discriminator are defined elsewhere as you mentioned

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize the GAN model
gan = GAN(in_channels=3, base_channels=64).to(device)

# Set up loss functions and optimizers
criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss for GANs
optimizer_g = optim.Adam(gan.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Generator optimizer
optimizer_d = optim.Adam(gan.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Discriminator optimizer

# Load dataset (assuming your Dataset class is GhibliDataset)
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizing to [-1, 1]
])

train_dataset = GhibliDataset(root_dir='/path/to/dataset', mode='training', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Training loop
num_epochs = 100  # Set to the number of epochs you want to train
for epoch in range(num_epochs):
    for i, (real_images, ghibli_images) in enumerate(train_loader):
        real_images = real_images.to(device)  # Move to GPU if available
        ghibli_images = ghibli_images.to(device)

        # -------------------------
        #  Train Discriminator
        # -------------------------
        optimizer_d.zero_grad()

        # Discriminator on real images
        real_labels = torch.ones(real_images.size(0), 1).to(device)  # Real labels for real images
        fake_labels = torch.zeros(real_images.size(0), 1).to(device)  # Fake labels for generated images

        real_output = gan.discriminator(real_images)  # Real image prediction from discriminator
        d_loss_real = criterion(real_output, real_labels)  # Loss on real images

        # Discriminator on fake images (generated by the generator)
        fake_images = gan.generator(real_images)  # Generated fake Ghibli images from generator
        fake_output = gan.discriminator(fake_images.detach())  # Pass through discriminator, detach to not update generator
        d_loss_fake = criterion(fake_output, fake_labels)  # Loss on fake images

        # Total discriminator loss
        d_loss = (d_loss_real + d_loss_fake) / 2
        d_loss.backward()  # Backpropagate discriminator loss
        optimizer_d.step()  # Update discriminator weights

        # -------------------------
        #  Train Generator
        # -------------------------
        optimizer_g.zero_grad()

        # Generator wants to fool the discriminator, so labels for fake images are 'real' (1)
        fake_output = gan.discriminator(fake_images)  # Pass fake images through the discriminator
        g_loss = criterion(fake_output, real_labels)  # Generator loss (discriminator thinks fake is real)

        g_loss.backward()  # Backpropagate generator loss
        optimizer_g.step()  # Update generator weights

        # Print statistics
        if i % 50 == 0:  # Print every 50 batches
            print(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{len(train_loader)}], '
                  f'Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}')

    # Save the model at the end of each epoch (optional)
    torch.save(gan.state_dict(), f'gan_epoch_{epoch}.pth')

    # Optional: Save generated images every epoch or at specific intervals
    # save_generated_images(fake_images, epoch)

l1_loss_fn = nn.L1Loss()
bce_loss_fn = nn.BCELoss()

lambda_l1 = 100  # weight for L1 loss (tunable)

def train_gan(generator, discriminator, dataloader, num_epochs, device):
    generator.to(device)
    discriminator.to(device)

    # Optimizers
    g_optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))
    d_optimizer = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))

    for epoch in range(num_epochs):
        for i, (input_image, target_image) in enumerate(dataloader):
            input_image = input_image.to(device)
            target_image = target_image.to(device)

            real_labels = torch.ones(input_image.size(0), 1, device=device)
            fake_labels = torch.zeros(input_image.size(0), 1, device=device)

            ####=======================
            # Train Discriminator
            ####=======================
            discriminator.zero_grad()

            # Real images
            real_output = discriminator(target_image)
            d_loss_real = bce_loss_fn(real_output, real_labels)

            # Fake images
            fake_image = generator(input_image).detach()  # detach to avoid generator gradients
            fake_output = discriminator(fake_image)
            d_loss_fake = bce_loss_fn(fake_output, fake_labels)

            d_loss = (d_loss_real + d_loss_fake) * 0.5
            d_loss.backward()
            d_optimizer.step()

            ####=======================
            # Train Generator
            ####=======================
            generator.zero_grad()

            generated_image = generator(input_image)
            fake_output = discriminator(generated_image)

            g_adv_loss = bce_loss_fn(fake_output, real_labels)
            g_l1_loss = l1_loss_fn(generated_image, target_image)
            g_loss = g_adv_loss + lambda_l1 * g_l1_loss

            g_loss.backward()
            g_optimizer.step()

            # Logging
            if i % 50 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], "
                      f"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, "
                      f"L1: {g_l1_loss.item():.4f}, Adv: {g_adv_loss.item():.4f}")
